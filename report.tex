\documentclass[12pt,a4paper]{article}

% ─── Packages ────────────────────────────────────────────────────────────────
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{parskip}

% ─── Colours ─────────────────────────────────────────────────────────────────
\definecolor{codeblue}{RGB}{0,83,156}
\definecolor{codegray}{RGB}{245,245,245}
\definecolor{commentgreen}{RGB}{34,139,34}

% ─── Code listings style ─────────────────────────────────────────────────────
\lstdefinestyle{gostyle}{
  backgroundcolor=\color{codegray},
  commentstyle=\color{commentgreen}\itshape,
  keywordstyle=\color{codeblue}\bfseries,
  stringstyle=\color{red!70!black},
  basicstyle=\ttfamily\small,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{gray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=4,
  frame=single,
  rulecolor=\color{gray!40},
  language=Go
}
\lstset{style=gostyle}

% ─── Page style ──────────────────────────────────────────────────────────────
\pagestyle{fancy}
\fancyhf{}
\rhead{GoGossip --- P2P Gossip Protocol}
\lhead{Computer Networks Project}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ─── Section formatting ───────────────────────────────────────────────────────
\titleformat{\section}{\large\bfseries\color{codeblue}}{{\thesection}}{1em}{}[\titlerule]
\titleformat{\subsection}{\normalsize\bfseries}{{\thesubsection}}{1em}{}

% ─── Hyperref setup ───────────────────────────────────────────────────────────
\hypersetup{
  colorlinks=true,
  linkcolor=codeblue,
  urlcolor=codeblue,
  citecolor=codeblue
}

% ─── Document ─────────────────────────────────────────────────────────────────
\begin{document}

% ══════════════════════════════════════════════════════
%  Title Page
% ══════════════════════════════════════════════════════
\begin{titlepage}
  \centering
  \vspace*{2cm}
  {\Huge\bfseries GoGossip\par}
  \vspace{0.5cm}
  {\large\itshape A Peer-to-Peer Gossip Protocol Implementation in Go\par}
  \vspace{2cm}
  \rule{0.8\textwidth}{0.4pt}\par
  \vspace{1cm}
  {\large Computer Networks Course --- Phase 5 Report\par}
  \vspace{0.5cm}
  {\large Design, Implementation Notes, Experimental Analysis\par}
  \vspace{2cm}
  \rule{0.8\textwidth}{0.4pt}\par
  \vfill
  {\large\today\par}
\end{titlepage}

\tableofcontents
\newpage

% ══════════════════════════════════════════════════════
%  1  Introduction
% ══════════════════════════════════════════════════════
\section{Introduction}

GoGossip is a fully decentralised peer-to-peer gossip network written in Go.
The system implements epidemic message dissemination over UDP, where nodes
spread messages to a small random subset of their neighbours (the \emph{fanout})
and the information propagates exponentially until the entire network is reached.

Key goals of the project:
\begin{itemize}[leftmargin=1.5em]
  \item Implement a functioning gossip overlay with dynamic peer discovery.
  \item Support both \emph{push} (immediate forwarding) and \emph{pull}
        (\texttt{IHAVE}/\texttt{IWANT}) dissemination strategies.
  \item Add Sybil resistance via Proof-of-Work on \texttt{HELLO} messages.
  \item Measure convergence time and message overhead as a function of
        network size, fanout, TTL, and neighbour-selection policy.
\end{itemize}

% ══════════════════════════════════════════════════════
%  2  System Design
% ══════════════════════════════════════════════════════
\section{System Design and Architecture}

\subsection{Package Structure}

The project is organised as a standard Go module.
Table~\ref{tab:packages} lists every internal package and its single responsibility.

\begin{table}[H]
\centering
\caption{Internal package responsibilities.}
\label{tab:packages}
\renewcommand{\arraystretch}{1.3}
\begin{tabularx}{\textwidth}{lX}
\toprule
\textbf{Package} & \textbf{Responsibility} \\
\midrule
\texttt{cmd}           & CLI entry-point; flag parsing; starts the node. \\
\texttt{node}          & Core node lifecycle (\texttt{New}, \texttt{Start}); configuration. \\
\texttt{node/router}   & Packet dispatch: decodes envelope and routes to the correct handler. \\
\texttt{node/handlers\_hello}  & \texttt{HELLO} send/receive, PoW validation. \\
\texttt{node/handlers\_peers}  & \texttt{GET\_PEERS} / \texttt{PEERS\_LIST} exchange. \\
\texttt{node/handlers\_gossip} & \texttt{GOSSIP} receive, publish, forward. \\
\texttt{node/handlers\_pull}   & \texttt{IHAVE} / \texttt{IWANT} and the pull loop. \\
\texttt{node/handlers\_ping}   & \texttt{PING} / \texttt{PONG} and the ping-prune loop. \\
\texttt{node/experiment}       & Writes JSON metric lines to the experiment log file. \\
\texttt{message}       & Envelope type, payload types, encode/decode. \\
\texttt{peer}          & \texttt{Peer} struct and \texttt{Store} (thread-safe peer list). \\
\texttt{seen}          & Thread-safe dedup set for gossip message IDs. \\
\texttt{cache}         & LRU-ordered gossip payload cache (serves \texttt{IWANT} responses). \\
\texttt{pow}           & SHA-256 Proof-of-Work: \texttt{Mine} and \texttt{Verify}. \\
\texttt{network}       & UDP \texttt{Listen} and \texttt{Send} helpers. \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Message Envelope}

Every UDP datagram carries a JSON-encoded \texttt{Envelope}:

\begin{lstlisting}[caption={Envelope fields (message/message.go).}]
type Envelope struct {
    Version    int             `json:"version"`
    MsgID      string          `json:"msg_id"`
    MsgType    MessageType     `json:"msg_type"`
    SenderID   string          `json:"sender_id"`
    SenderAddr string          `json:"sender_addr"`
    Timestamp  int64           `json:"timestamp_ms"`
    TTL        int             `json:"ttl"`
    Payload    json.RawMessage `json:"payload"`
}
\end{lstlisting}

Supported message types are listed in Table~\ref{tab:msgtypes}.

\begin{table}[H]
\centering
\caption{Message types and their purpose.}
\label{tab:msgtypes}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{llp{8cm}}
\toprule
\textbf{Type} & \textbf{Direction} & \textbf{Purpose} \\
\midrule
\texttt{HELLO}      & bidirectional & Announce presence; triggers peer registration. Carries PoW proof. \\
\texttt{GET\_PEERS} & request       & Ask a node for its peer list. \\
\texttt{PEERS\_LIST}& response      & Reply with up to \texttt{max\_peers} known peers. \\
\texttt{GOSSIP}     & broadcast     & Carry application data; forwarded up to \texttt{ttl} hops. \\
\texttt{PING}       & request       & Liveness probe. \\
\texttt{PONG}       & response      & Confirms the sender is alive; refreshes last-seen time. \\
\texttt{IHAVE}      & advertisement & Periodic list of message IDs this node holds. \\
\texttt{IWANT}      & request       & Request full payloads for IDs not yet seen. \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Node Lifecycle}

\begin{enumerate}[leftmargin=1.5em]
  \item \textbf{Startup:} \texttt{New(cfg)} creates the node, mines the PoW nonce
        (if \texttt{pow\_k > 0}), and initialises all internal data structures.
  \item \textbf{Bootstrap:} \texttt{Start()} sends \texttt{HELLO} followed by
        \texttt{GET\_PEERS} to the seed node to fill the peer list.
  \item \textbf{Push loop:} Every received \texttt{GOSSIP} with \texttt{TTL > 0}
        is immediately forwarded to \texttt{fanout} neighbours.
  \item \textbf{Ping loop:} A background goroutine sends \texttt{PING} to all
        peers at \texttt{ping\_interval} ms and prunes those that have not
        replied with \texttt{PONG} within \texttt{peer\_timeout} ms.
  \item \textbf{Pull loop} (optional): Every \texttt{pull\_interval} ms,
        advertise the \texttt{ihave\_max\_ids} most-recently-seen message IDs
        to \texttt{fanout} neighbours via \texttt{IHAVE}.
\end{enumerate}

% ══════════════════════════════════════════════════════
%  3  Implementation Notes
% ══════════════════════════════════════════════════════
\section{Key Implementation Notes}

\subsection{Push Gossip (Epidemic Forwarding)}

When a node receives a \texttt{GOSSIP} envelope it:
\begin{enumerate}[leftmargin=1.5em]
  \item Computes the dedup key \texttt{origin\_id:origin\_timestamp\_ms}.
  \item Checks the \texttt{seen.Set}; if already seen, silently drops.
  \item Marks the key as seen, adds the payload to the gossip cache.
  \item If \texttt{TTL > 0}, calls \texttt{forwardGossip} which selects
        up to \texttt{fanout} peers (excluding the sender) and re-sends
        the envelope with \texttt{TTL-1}.
\end{enumerate}

The \texttt{NeighborsPolicy} flag controls peer selection:
\begin{itemize}[leftmargin=1.5em]
  \item \texttt{first}: always picks the first \texttt{fanout} entries in
        insertion order (deterministic, low variance).
  \item \texttt{random}: shuffles the peer list before selection using a
        seeded local \texttt{*rand.Rand} (avoids the deprecated global
        \texttt{rand.Seed}).
\end{itemize}

\subsection{Pull Protocol (IHAVE / IWANT)}

The pull mechanism provides \emph{repair}: nodes that missed a pushed message
can recover it later.

\begin{enumerate}[leftmargin=1.5em]
  \item The pull loop fires every \texttt{pull\_interval} ms and broadcasts
        \texttt{IHAVE\{IDs: [id1, id2, \ldots]\}} to \texttt{fanout} neighbours.
  \item A receiver compares the advertised IDs against its own \texttt{seen.Set};
        for any missing IDs it sends \texttt{IWANT}.
  \item The original sender looks up each requested ID in the
        \texttt{cache.GossipCache} and replies with a \texttt{GOSSIP}
        envelope carrying \texttt{TTL=0} (so the receiver stores it but
        does not re-forward).
\end{enumerate}

The \texttt{cache.GossipCache} maintains insertion order so the most-recent
\texttt{N} IDs can be cheaply obtained for \texttt{IHAVE} advertisements,
while also enabling \(\mathcal{O}(1)\) payload lookup for \texttt{IWANT}.

\subsection{Proof-of-Work Sybil Resistance}

To make Sybil attacks costly, every node must solve a PoW puzzle before
being admitted to the network.

\paragraph{Protocol.}
When joining, a node finds a nonce such that:
\[
  \text{SHA-256}(\textit{node\_id} \,\|\, \textit{nonce})
  \;\text{starts with } k \text{ hex-zero digits}
\]
The proof is attached to the \texttt{HELLO} payload:

\begin{lstlisting}[caption={PoW proof fields in the HELLO payload.}]
"pow": {
    "hash_alg":    "sha256",
    "difficulty_k": 4,
    "nonce":        106283,
    "digest_hex":   "00002a9db661138e07c5235b8ad6c1b7..."
}
\end{lstlisting}

\paragraph{Acceptance rules.}
On receiving \texttt{HELLO}, the node:
\begin{enumerate}[leftmargin=1.5em]
  \item Rejects the message if the \texttt{pow} field is missing.
  \item Rejects if \texttt{difficulty\_k} $\neq$ the locally configured
        \texttt{pow\_k}.
  \item Computes \(\text{SHA-256}(\textit{sender\_id} \,\|\, \textit{nonce})\)
        and verifies it equals \texttt{digest\_hex} and starts with $k$ zeros.
  \item Only then adds the sender to the peer list.
\end{enumerate}

\paragraph{Cost.}
With $k=4$ (default), approximately $16^4 / 2 = 32\,768$ hashes are needed
on average. On a typical laptop this takes \textasciitilde30--80 ms ---
expensive enough to deter large-scale Sybil attacks but negligible for
legitimate nodes.

\subsection{Thread Safety}

All shared state uses fine-grained locking:
\begin{itemize}[leftmargin=1.5em]
  \item \texttt{peer.Store}: \texttt{sync.RWMutex} on peer map.
  \item \texttt{seen.Set}: \texttt{sync.RWMutex} on ID map.
  \item \texttt{cache.GossipCache}: \texttt{sync.RWMutex}; readers hold a
        read lock, writers hold a write lock.
  \item \texttt{expLogMu}: dedicated \texttt{sync.Mutex} serialises writes
        to the experiment JSON log file.
\end{itemize}

% ══════════════════════════════════════════════════════
%  4  Evaluation Methodology
% ══════════════════════════════════════════════════════
\section{Evaluation Methodology}

\subsection{Data Collection}

Each node appends JSON-encoded metric lines to a configurable experiment log
file (\texttt{--experiment-log} flag). Two event types are recorded:

\begin{itemize}[leftmargin=1.5em]
  \item \texttt{gossip\_publish}: emitted by the origin node at time $t_0$.
  \item \texttt{gossip\_recv}: emitted by every receiving node with the
        wall-clock timestamp \texttt{recv\_ms}.
  \item \texttt{msg\_sent}: emitted for every outgoing envelope (any type),
        enabling total message overhead counting.
\end{itemize}

\subsection{Metrics}

\paragraph{Convergence Time.}
Let $t_0$ be the publication timestamp of a gossip message and $t_i$ the
reception timestamp at node $i$. The convergence time is defined as the
minimum $T$ such that at least 95\% of nodes have received the message by
$t_0 + T$:
\[
  T_{\text{conv}} = \min \left\{ T \;\Big|\;
    \frac{\left|\{i : t_i - t_0 \leq T\}\right|}{N} \geq 0.95
  \right\}
\]

\paragraph{Message Overhead.}
The total number of messages sent by all nodes from $t_0$ until 95\%
coverage is reached, summing all message types (\texttt{GOSSIP},
\texttt{HELLO}, \texttt{GET\_PEERS}, \texttt{PEERS\_LIST}, \texttt{PING},
\texttt{PONG}).

\subsection{Experimental Setup}

Each configuration was repeated 5 times with different random seeds.
Results were aggregated (mean $\pm$ standard deviation) across runs.
Table~\ref{tab:params} summarises the parameter sweep.

\begin{table}[H]
\centering
\caption{Experimental parameters.}
\label{tab:params}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lll}
\toprule
\textbf{Experiment} & \textbf{Varied} & \textbf{Fixed} \\
\midrule
Network size  & $N \in \{10, 20, 50\}$      & fanout=3, TTL=10, policy=first \\
Fanout        & fanout $\in \{2, 3, 5\}$    & $N \in \{10,20,50\}$, TTL=10 \\
TTL           & TTL $\in \{5, 10, 20\}$     & $N \in \{10,20,50\}$, fanout=3 \\
Policy        & \{first, random\}           & $N \in \{10,20,50\}$, fanout=3 \\
Push vs Pull  & pull\_interval $\in \{0, 1000\}$ ms & $N \in \{10,20,50\}$ \\
\bottomrule
\end{tabular}
\end{table}

% ══════════════════════════════════════════════════════
%  5  Results and Analysis
% ══════════════════════════════════════════════════════
\section{Results and Analysis}

\subsection{Effect of Network Size (N)}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.92\textwidth]{experiment_results/plot_n_seed.png}
  \caption{Convergence time and message overhead as a function of network
           size $N$, comparing push-only vs.\ push+pull configurations
           (fanout=3, TTL=10, policy=first).}
  \label{fig:n_seed}
\end{figure}

Figure~\ref{fig:n_seed} shows the dominant effect: both convergence time and
overhead grow with $N$.

\paragraph{Push-only ($\texttt{pull\_interval}=0$).}
Convergence is remarkably fast for all tested sizes:
$\approx 2$--3\,ms at $N=10$, $\approx 6$\,ms at $N=20$, and
$\approx 48$--55\,ms at $N=50$. This is consistent with the theoretical
$\mathcal{O}(\log N)$ hop-count of epidemic protocols.

\paragraph{Push+Pull ($\texttt{pull\_interval}=1000$\,ms).}
Adding the pull mechanism with a 1-second timer \emph{harms} convergence time
for most runs because nodes must wait up to 1000\,ms before the \texttt{IHAVE}
cycle fires. For $N=20$ this results in $\sim 600$\,ms median convergence
(up from 6\,ms), and for $N=50$ several runs show convergence over 1 second.
Overhead also rises substantially due to the additional \texttt{IHAVE}/
\texttt{IWANT}/\texttt{GOSSIP} exchange traffic.

\textbf{Takeaway:} The pull mechanism is not beneficial when push alone achieves
near-perfect coverage. Its value lies in \emph{repair}: recovering messages
that the push phase missed in lossy or high-churn networks, which was not
simulated here.

\subsection{Effect of Fanout}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.92\textwidth]{experiment_results/plot_fanout.png}
  \caption{Convergence time and message overhead versus fanout
           for $N \in \{10, 20, 50\}$ (TTL=10, policy=first, push-only).}
  \label{fig:fanout}
\end{figure}

Figure~\ref{fig:fanout} presents the trade-off between fanout, speed, and cost.

At $N=10$ the differences are negligible: the network is so small that all
configurations converge in 2--5\,ms regardless of fanout. At $N=50$ the
trend becomes visible:

\begin{itemize}[leftmargin=1.5em]
  \item \textbf{Convergence time} decreases modestly with higher fanout
        (fanout=5 achieves $\sim 44$--61\,ms, comparable to fanout=2 at
        46--64\,ms). The improvement is not dramatic because TTL=10 already
        provides enough forwarding budget for full coverage.
  \item \textbf{Message overhead} grows with fanout, as each node contacts
        more peers per hop. fanout=5 generates roughly 10--30\% more messages
        than fanout=2 at $N=50$.
\end{itemize}

\textbf{Takeaway:} Increasing fanout beyond 3 yields diminishing returns on
convergence speed while linearly increasing overhead. Fanout=3 is a good
default for small-to-medium networks.

\subsection{Effect of TTL}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.92\textwidth]{experiment_results/plot_ttl.png}
  \caption{Convergence time and message overhead versus TTL
           for $N \in \{10, 20, 50\}$ (fanout=3, policy=first, push-only).}
  \label{fig:ttl}
\end{figure}

Figure~\ref{fig:ttl} reveals an important result: \textbf{TTL has surprisingly
little effect on convergence time} across the tested values.

At $N=50$, convergence with TTL=5 ($\approx$\,49\,ms) is nearly identical
to TTL=20 ($\approx$\,52\,ms). This occurs because with fanout=3 and
$N=50$, 5 hops ($3^5 = 243$) are already sufficient to cover the whole
network. Beyond the critical TTL, additional hops only increase overhead
without improving coverage.

Overhead grows slightly with TTL because more duplicate forwards are permitted
before the TTL counter reaches zero and forwarding stops. However the
difference between TTL=5 and TTL=20 is modest (\textasciitilde10--15\%), as
the \texttt{seen.Set} dedup prevents most redundant processing.

\textbf{Takeaway:} TTL should be set to $\lceil \log_{\text{fanout}} N \rceil$
plus a small safety margin. Larger TTL values waste overhead without benefit.

\subsection{Effect of Neighbour Selection Policy}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.92\textwidth]{experiment_results/plot_policy.png}
  \caption{Convergence time and message overhead for \texttt{first} vs.\
           \texttt{random} neighbour-selection policy
           ($N \in \{10, 20, 50\}$, fanout=3, TTL=10, push-only).}
  \label{fig:policy}
\end{figure}

Figure~\ref{fig:policy} compares the two neighbour-selection strategies.

At $N=50$, the \texttt{first} policy achieves slightly lower median convergence
time ($\approx 47$\,ms) and lower overhead ($\approx 1866$ messages) compared
to \texttt{random} ($\approx 56$\,ms, $\approx 2232$ messages). However, the
gap is within the run-to-run variance, suggesting the two policies are
statistically equivalent at these scales.

In theory, \texttt{random} should reduce the probability of a message becoming
trapped in a densely-connected clique and should produce more uniform coverage.
The \texttt{first} policy is more deterministic and cache-friendly (no shuffle
allocation), making it the better default for a local-network deployment.

\textbf{Takeaway:} Policy has minimal impact at small to medium network
sizes. For large heterogeneous networks, \texttt{random} may provide more
robust coverage at the cost of slightly higher overhead.

\subsection{Summary Table}

\begin{table}[H]
\centering
\caption{Mean convergence time (ms) and mean message overhead across 5 runs.
         Push-only configuration.}
\label{tab:summary}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lcrr}
\toprule
\textbf{Experiment} & $N$ & \textbf{Conv.\ (ms)} & \textbf{Overhead} \\
\midrule
\multirow{3}{*}{Baseline (fanout=3, TTL=10, first)}
  & 10  &  2.4  &   60  \\
  & 20  &  5.8  &  213  \\
  & 50  & 49.2  & 1967  \\
\midrule
fanout=2 (N=50)   & 50 & 53.6 & 2122 \\
fanout=3 (N=50)   & 50 & 49.4 & 1995 \\
fanout=5 (N=50)   & 50 & 45.8 & 1832 \\
\midrule
TTL=5  (N=50)     & 50 & 49.0 & 1934 \\
TTL=10 (N=50)     & 50 & 51.0 & 2075 \\
TTL=20 (N=50)     & 50 & 51.6 & 1941 \\
\midrule
policy=first  (N=50) & 50 & 47.6 & 1866 \\
policy=random (N=50) & 50 & 56.0 & 2233 \\
\bottomrule
\end{tabular}
\end{table}

% ══════════════════════════════════════════════════════
%  6  Conclusion
% ══════════════════════════════════════════════════════
\section{Conclusion}

GoGossip demonstrates that even a straightforward epidemic protocol achieves
very fast convergence on small-to-medium networks. The key findings are:

\begin{enumerate}[leftmargin=1.5em]
  \item \textbf{Network size} is the dominant factor: convergence scales as
        roughly $\mathcal{O}(\log N)$ in time and $\mathcal{O}(N)$ in
        overhead, consistent with theory.
  \item \textbf{Fanout} provides a clear overhead--speed trade-off.
        Fanout=3 is a practical sweet spot.
  \item \textbf{TTL} matters only up to the critical threshold
        $\lceil\log_{\text{fanout}} N\rceil$; setting it higher wastes
        bandwidth.
  \item \textbf{Neighbour policy} has negligible impact at tested scales.
  \item \textbf{Pull mechanism} is harmful to latency when push alone
        achieves full coverage, but would be beneficial in lossy networks.
  \item \textbf{Proof-of-Work} (difficulty $k=4$, SHA-256) provides
        practical Sybil resistance with $\approx$30--80\,ms mining cost
        per node on commodity hardware.
\end{enumerate}

\end{document}
